{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A3 - NLP for Yelp and IMDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import os.path\n",
    "import re\n",
    "import numpy as np\n",
    "import string\n",
    "from sklearn import metrics\n",
    "from scipy.sparse import lil_matrix\n",
    "from scipy.sparse import vstack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Functions to open files\n",
    "def open_with_panda(filename):\n",
    "    return pd.read_csv(filename, sep='\\t', names=[\"Comment\", \"Evaluation\"]).as_matrix()\n",
    "\n",
    "\n",
    "def open_vocab_file(filename):\n",
    "    return pd.read_csv(yelp_vocab_file, sep='\\t', header=None, skip_blank_lines=True).as_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function to generate encodings\n",
    "def encode_reviews(file_to_create, X, Y, vocabFile, overwrite=False):\n",
    "    if not (os.path.isfile(file_to_create)) or overwrite:\n",
    "        # Grab vocab encodings\n",
    "        vocab = open_vocab_file(vocabFile)\n",
    "        vocab_dict = dict([(vocab[i][0], int(vocab[i][1])) for i in range(len(vocab))])\n",
    "        list_to_write = []\n",
    "\n",
    "        cleaned_reviews = [re.sub(r'[^\\w\\s\\d]|_','',X[i].lower()) for i in range(len(X))]\n",
    "        final_reviews = [re.sub(r'\\s+',' ',cleaned_reviews[i]).strip() for i in range(len(cleaned_reviews))]\n",
    "\n",
    "        for i in range(len(final_reviews)):\n",
    "            build_string = \"\"\n",
    "            for x in final_reviews[i].split(\" \"):\n",
    "                if x in vocab_dict.keys():\n",
    "                    build_string += str(vocab_dict[x]) + \" \"\n",
    "            list_to_write.append(build_string.strip() + \"\\t\" + str(Y[i]))\n",
    "\n",
    "        with open(file_to_create,'w') as f:\n",
    "            f.write(\"\\n\".join(list_to_write))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function to generate vocab\n",
    "def vocabulary(file, reviews, overwrite=False):\n",
    "    if not (os.path.isfile(file)) or overwrite:\n",
    "        # Let's create the vocab.txt file\n",
    "        cleaned_reviews = [re.sub(r'[^\\w\\s\\d]|_','',reviews[i].lower()) for i in range(len(reviews))]\n",
    "        final_reviews = [re.sub(r'\\s+',' ',cleaned_reviews[i]).strip() for i in range(len(cleaned_reviews))]\n",
    "\n",
    "        found_punct = False\n",
    "        for c in string.punctuation:\n",
    "            for rev in final_reviews:\n",
    "                if c in rev:\n",
    "                    # print(rev + \"\\n\")\n",
    "                    found_punct = True\n",
    "\n",
    "        if found_punct:\n",
    "            raise Exception('Found punctuation')\n",
    "\n",
    "        words = \" \".join(final_reviews).split(\" \")\n",
    "        unique_words, count = np.unique(words, return_counts=True)\n",
    "        unique_words_array = np.asarray((unique_words,count)).T\n",
    "        frequency_unique_words = np.flip(unique_words_array[unique_words_array[:,-1].astype(int).argsort()],0)\n",
    "\n",
    "        with open(file,'w') as f:\n",
    "            for i in range(10000):\n",
    "                f.write(\n",
    "                    frequency_unique_words[i][0] + \"\\t\" +\n",
    "                    str(i) + \"\\t\" +\n",
    "                    frequency_unique_words[i][-1] + \"\\n\"\n",
    "                )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# File paths\n",
    "\n",
    "yelp_train_file = \"hwk3_datasets/yelp-train.txt\"\n",
    "yelp_valid_file = \"hwk3_datasets/yelp-valid.txt\"\n",
    "yelp_test_file = \"hwk3_datasets/yelp-test.txt\"\n",
    "\n",
    "imdb_valid_file = \"hwk3_datasets/IMDB-valid.txt\"\n",
    "imdb_test_file = \"hwk3_datasets/IMDB-test.txt\"\n",
    "imdb_train_file = \"hwk3_datasets/IMDB-train.txt\"\n",
    "\n",
    "yelp_vocab_file = \"data/yelp-vocab.txt\"\n",
    "imdb_vocab_file = \"data/IMDB-vocab.txt\"\n",
    "\n",
    "wr_yelp_train_file = \"data/yelp-train.txt\"\n",
    "wr_yelp_valid_file = \"data/yelp-valid.txt\"\n",
    "wr_yelp_test_file = \"data/yelp-test.txt\"\n",
    "\n",
    "wr_imdb_valid_file = \"data/IMDB-valid.txt\"\n",
    "wr_imdb_test_file = \"data/IMDB-test.txt\"\n",
    "wr_imdb_train_file = \"data/IMDB-train.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Training data\n",
    "\n",
    "data_yelp_train = open_with_panda(yelp_train_file)\n",
    "yelp_train_X = data_yelp_train[:,0]\n",
    "yelp_train_Y = data_yelp_train[:,-1]\n",
    "\n",
    "data_imdb_train = open_with_panda(imdb_train_file)\n",
    "imdb_train_X = data_imdb_train[:,0]\n",
    "imdb_train_Y = data_imdb_train[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Generate vocab\n",
    "vocabulary(yelp_vocab_file, yelp_train_X.flatten())\n",
    "vocabulary(imdb_vocab_file, imdb_train_X.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Encode yelp reviews\n",
    "\n",
    "encode_reviews(wr_yelp_train_file, yelp_train_X, yelp_train_Y, yelp_vocab_file)\n",
    "\n",
    "data_yelp_valid = open_with_panda(yelp_valid_file)\n",
    "data_yelp_test = open_with_panda(yelp_test_file)\n",
    "\n",
    "encode_reviews(wr_yelp_valid_file, data_yelp_valid[:,0], data_yelp_valid[:,-1], yelp_vocab_file)\n",
    "encode_reviews(wr_yelp_test_file, data_yelp_test[:,0], data_yelp_test[:,-1], yelp_vocab_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Encode IMDB reviews\n",
    "encode_reviews(wr_imdb_train_file, imdb_train_X, imdb_train_Y, imdb_vocab_file)\n",
    "data_imdb_valid = open_with_panda(imdb_valid_file)\n",
    "data_imdb_test = open_with_panda(imdb_test_file)\n",
    "\n",
    "encode_reviews(wr_imdb_valid_file, data_imdb_valid[:,0], data_imdb_valid[:,-1], imdb_vocab_file)\n",
    "encode_reviews(wr_imdb_test_file, data_imdb_test[:,0], data_imdb_test[:,-1], imdb_vocab_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function - unif classifier\n",
    "def uniform_classifier(classes, y_test):\n",
    "    from random import randint\n",
    "    l = []\n",
    "    for i in range(len(y_test)):\n",
    "        rand = randint(0, len(classes)-1)\n",
    "        l.append(classes[rand])\n",
    "    y_pred = np.array(l, dtype=np.int16)\n",
    "    y_test = np.array(y_test, dtype=np.int16)\n",
    "    return metrics.f1_score(y_test, y_pred, average=None) # Get F1 score for each of the classes, thus Average=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function - majority classifier\n",
    "def majority_classifier(classes, y_train, y_test):\n",
    "    counter = dict([(classes[i], 0) for i in range(len(classes))])\n",
    "\n",
    "    for y in y_train:\n",
    "        counter[y]+=1\n",
    "\n",
    "    majority_class = max(counter, key=counter.get)\n",
    "    y_pred = np.full((len(y_test),1), majority_class, dtype=np.int16)\n",
    "    y_test = np.array(y_test, dtype=np.int16)\n",
    "    \n",
    "    return metrics.f1_score(y_test, y_pred, average=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Binary bag of words: read encoding file and generate\n",
    "def binary_bag_of_words(filename):\n",
    "    features = 10000\n",
    "    with open(filename, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        X = lil_matrix((len(lines), features), dtype=np.int32)\n",
    "        for i in range(len(lines)):\n",
    "            encodings = lines[i].split(\" \")\n",
    "            encodings[-1] = encodings[-1].split(\"\\t\")[0]\n",
    "\n",
    "            if len(encodings) == 1 and encodings[0] == '':\n",
    "                continue  # We have only 1 encoding, which means the review has no vocab words\n",
    "\n",
    "            for j in range(len(encodings)):\n",
    "                e = int(encodings[j])\n",
    "                X[i,e] = 1\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Frequency bag of words - with division\n",
    "def frequency_bag_of_words(filename):\n",
    "    features = 10000\n",
    "    with open(filename, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        X = lil_matrix((len(lines), features), dtype=np.int32)\n",
    "        for i in range(len(lines)):\n",
    "            encodings = lines[i].split(\" \")\n",
    "            encodings[-1] = encodings[-1].split(\"\\t\")[0]\n",
    "            \n",
    "            if len(encodings) == 1 and encodings[0] == '':\n",
    "                continue  # We have only 1 encoding, which means the review has no vocab words\n",
    "            \n",
    "            for j in range(len(encodings)):\n",
    "                e = int(encodings[j])\n",
    "                X[i,e] += 1\n",
    "        sum_vector = X.sum(axis=1)\n",
    "        for i in range(len(sum_vector)):\n",
    "            if sum_vector[i] == 0:\n",
    "                sum_vector[i] = 1  #just so we don't divide by 0\n",
    "\n",
    "        return lil_matrix(X / sum_vector, dtype=np.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "yelp_classes = [i for i in range(1,6)]\n",
    "imdb_classes = [0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.07279029  0.15224913  0.15273775  0.25341841  0.27134725]\n",
      "[ 0.         0.         0.         0.5196151  0.       ]\n",
      "0.180508568834\n",
      "0.103923019985\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "unif = uniform_classifier(yelp_classes, data_yelp_test[:,-1].flatten())\n",
    "maj = majority_classifier(yelp_classes, yelp_train_Y, data_yelp_test[:,-1].flatten()) # Will get an F1 error, since we'll have 0/0 in some cases\n",
    "\n",
    "print(unif)\n",
    "print(maj)\n",
    "print(sum(unif)/len(unif))\n",
    "print(sum(maj)/len(maj))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Yelp Binary Bag of Words\n",
    "\n",
    "binary_bag_train = binary_bag_of_words(wr_yelp_train_file)\n",
    "binary_bag_valid = binary_bag_of_words(wr_yelp_valid_file)\n",
    "binary_bag_test = binary_bag_of_words(wr_yelp_test_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get data ready for cross gridsearchCV with validation data\n",
    "\n",
    "yelp_train_valid_X = vstack([binary_bag_train, binary_bag_valid])\n",
    "train_test_fold = np.full((binary_bag_train.shape[0]), 0)\n",
    "valid_test_fold = np.full((binary_bag_valid.shape[0]), 1)\n",
    "test_fold = np.append(train_test_fold, valid_test_fold, axis=0)\n",
    "\n",
    "yelp_valid_Y = np.array(data_yelp_valid[:,-1], dtype=np.int32)\n",
    "\n",
    "yelp_train_valid_Y = np.array(np.append(yelp_train_Y, yelp_valid_Y, axis=0), dtype=np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naive Bayes for Yelp Binary Bag of Words\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import PredefinedSplit\n",
    "\n",
    "# clf = BernoulliNB()\n",
    "# tuned_parameters = [{'alpha': np.arange(0.01, 1.01, 0.01)}]\n",
    "# ps = PredefinedSplit(test_fold=test_fold)\n",
    "\n",
    "# clf = GridSearchCV(clf, tuned_parameters, cv=ps, refit=True)\n",
    "# clf.fit(yelp_train_valid_X, yelp_train_valid_Y)\n",
    "\n",
    "# y_pred = clf.predict(binary_bag_test)\n",
    "maximum = -1\n",
    "bestAlpha = -1\n",
    "yelp_train_Y = np.array(yelp_train_Y, dtype=np.int32)\n",
    "\n",
    "# for al in np.arange(0.01, 1.01, 0.01):\n",
    "#     clf = BernoulliNB(alpha=al)\n",
    "#     clf.fit(binary_bag_train, yelp_train_Y)\n",
    "#     y_pred_train = clf.predict(binary_bag_train)\n",
    "#     y_pred_valid = clf.predict(binary_bag_valid)\n",
    "#     f1_t = metrics.f1_score(yelp_train_Y,y_pred_train, average=None)\n",
    "#     f1_v = metrics.f1_score(yelp_valid_Y,y_pred_valid, average=None)\n",
    "#     print(al)\n",
    "#     print(np.mean(f1_t) + np.mean(f1_v))\n",
    "#     if np.mean(f1_t) + np.mean(f1_v) > maximum:\n",
    "#         maximum = np.mean(f1_t) + np.mean(f1_v)\n",
    "#         bestAlpha = al\n",
    "clf = BernoulliNB(alpha=bestAlpha)\n",
    "clf.fit(binary_bag_train, yelp_train_Y)\n",
    "y_pred = clf.predict(binary_bag_test)\n",
    "# print(clf.best_estimator_)\n",
    "# print(metrics.f1_score(np.array(data_yelp_test[:,-1].flatten(), dtype=np.int32), y_pred, average=None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1\n",
      "[ 0.42798354  0.27355623  0.24254473  0.44773907  0.52538071]\n",
      "0.383440855669\n"
     ]
    }
   ],
   "source": [
    "print(bestAlpha)\n",
    "f1 = metrics.f1_score(np.array(data_yelp_test[:,-1].flatten(), dtype=np.int32), y_pred, average=None)\n",
    "print(f1)\n",
    "print(np.mean(f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.52702703  0.30927835  0.32793522  0.51590595  0.61371351]\n",
      "0.5145\n",
      "LinearSVC(C=0.02, class_weight=None, dual=True, fit_intercept=True,\n",
      "     intercept_scaling=1, loss='hinge', max_iter=1000, multi_class='ovr',\n",
      "     penalty='l2', random_state=None, tol=0.0001, verbose=0)\n"
     ]
    }
   ],
   "source": [
    "# Linear SVC/SVM\n",
    "from sklearn.svm import LinearSVC\n",
    "clf = LinearSVC()\n",
    "\n",
    "# We have 10000 features and 8000 in training + valid; so solve dual\n",
    "tuned_parameters = [\n",
    "    {'penalty': ['l2'], \n",
    "     'loss':['squared_hinge', 'hinge'], \n",
    "     'dual':[True], \n",
    "    'C':np.arange(0.01, 1.01, 0.01),\n",
    "    }]\n",
    "ps = PredefinedSplit(test_fold=test_fold)\n",
    "\n",
    "clf = GridSearchCV(clf, tuned_parameters, cv=ps, refit=True)\n",
    "clf.fit(yelp_train_valid_X, yelp_train_valid_Y)\n",
    "\n",
    "y_pred = clf.predict(binary_bag_test)\n",
    "\n",
    "print(metrics.f1_score(np.array(data_yelp_test[:,-1].flatten(), dtype=np.int32), y_pred, average=None))\n",
    "print(metrics.accuracy_score(np.array(data_yelp_test[:,-1].flatten(), dtype=np.int32), y_pred))\n",
    "print(clf.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.17021277  0.14534884  0.19649123  0.38294993  0.43589744]\n",
      "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='best')\n"
     ]
    }
   ],
   "source": [
    "# Decision tree for yelp binary bag of words\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "clf = DecisionTreeClassifier()\n",
    "\n",
    "tuned_parameters = [\n",
    "    {'criterion': ['gini','entropy'],\n",
    "#      'max_depth':[4,5,6,7,8,9,10,11,12,15,20,30,40,50,70,90,120,150,None]\n",
    "    }]\n",
    "ps = PredefinedSplit(test_fold=test_fold)\n",
    "\n",
    "clf = GridSearchCV(clf, tuned_parameters, cv=ps, refit=True)\n",
    "clf.fit(yelp_train_valid_X, yelp_train_valid_Y)\n",
    "\n",
    "y_pred = clf.predict(binary_bag_test)\n",
    "\n",
    "print(metrics.f1_score(np.array(data_yelp_test[:,-1].flatten(), dtype=np.int32), y_pred, average=None))\n",
    "print(clf.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Yelp frequency bag of words\n",
    "freq_bag_train = frequency_bag_of_words(wr_yelp_train_file)\n",
    "freq_bag_valid = frequency_bag_of_words(wr_yelp_valid_file)\n",
    "freq_bag_test = frequency_bag_of_words(wr_yelp_test_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get data ready for cross gridsearchCV with validation data\n",
    "\n",
    "yelp_train_valid_X = vstack([freq_bag_train, freq_bag_valid])\n",
    "train_test_fold = np.full((freq_bag_train.shape[0]), 0)\n",
    "valid_test_fold = np.full((freq_bag_valid.shape[0]), 1)\n",
    "test_fold = np.append(train_test_fold, valid_test_fold, axis=0)\n",
    "\n",
    "yelp_valid_Y = np.array(data_yelp_valid[:,-1], dtype=np.int32)\n",
    "\n",
    "yelp_train_valid_Y = np.array(np.append(yelp_train_Y, yelp_valid_Y, axis=0), dtype=np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GaussianNB(priors=None)\n",
      "[ 0.17940199  0.140625    0.21021021  0.33888487  0.37697161]\n"
     ]
    }
   ],
   "source": [
    "# Naive Bayes for Yelp Binary Bag of Words\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "clf = GaussianNB()\n",
    "tuned_parameters = [{}]\n",
    "ps = PredefinedSplit(test_fold=test_fold)\n",
    "\n",
    "clf = GridSearchCV(clf, tuned_parameters, cv=ps, refit=True)\n",
    "clf.fit(yelp_train_valid_X.todense(), yelp_train_valid_Y)\n",
    "\n",
    "y_pred = clf.predict(freq_bag_test.todense())\n",
    "\n",
    "print(clf.best_estimator_)\n",
    "print(metrics.f1_score(np.array(data_yelp_test[:,-1].flatten(), dtype=np.int32), y_pred, average=None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearSVC(C=0.17000000000000001, class_weight=None, dual=True,\n",
      "     fit_intercept=True, intercept_scaling=1, loss='hinge', max_iter=1000,\n",
      "     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
      "     verbose=0)\n",
      "[ 0.41860465  0.21960784  0.16161616  0.42801556  0.62236988]\n"
     ]
    }
   ],
   "source": [
    "# Linear SVC/SVM\n",
    "from sklearn.svm import LinearSVC\n",
    "clf = LinearSVC()\n",
    "\n",
    "# We have 10000 features and 8000 in training + valid; so solve dual\n",
    "tuned_parameters = [\n",
    "    {'penalty': ['l2'], \n",
    "     'loss':['squared_hinge', 'hinge'], \n",
    "     'dual':[True], \n",
    "    'C':np.arange(0.01, 1.01, 0.01),\n",
    "    }]\n",
    "ps = PredefinedSplit(test_fold=test_fold)\n",
    "\n",
    "clf = GridSearchCV(clf, tuned_parameters, cv=ps, refit=True)\n",
    "clf.fit(yelp_train_valid_X, yelp_train_valid_Y)\n",
    "\n",
    "y_pred = clf.predict(freq_bag_test)\n",
    "\n",
    "print(clf.best_estimator_)\n",
    "print(metrics.f1_score(np.array(data_yelp_test[:,-1].flatten(), dtype=np.int32), y_pred, average=None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.22875817  0.13031161  0.18671454  0.38787024  0.44070278]\n",
      "DecisionTreeClassifier(class_weight=None, criterion='entropy', max_depth=None,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='best')\n"
     ]
    }
   ],
   "source": [
    "# Decision tree for yelp binary bag of words\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "clf = DecisionTreeClassifier()\n",
    "\n",
    "tuned_parameters = [\n",
    "    {'criterion': ['gini','entropy'],\n",
    "#      'max_depth':[4,5,6,7,8,9,10,11,12,15,20,30,40,50,70,90,120,150,None]\n",
    "    }]\n",
    "ps = PredefinedSplit(test_fold=test_fold)\n",
    "\n",
    "clf = GridSearchCV(clf, tuned_parameters, cv=ps, refit=True)\n",
    "clf.fit(yelp_train_valid_X, yelp_train_valid_Y)\n",
    "\n",
    "y_pred = clf.predict(freq_bag_test)\n",
    "\n",
    "print(metrics.f1_score(np.array(data_yelp_test[:,-1].flatten(), dtype=np.int32), y_pred, average=None))\n",
    "print(clf.best_estimator_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.50151976  0.50136022]\n"
     ]
    }
   ],
   "source": [
    "unif_imdb = uniform_classifier(imdb_classes, data_imdb_test[:,-1].flatten())\n",
    "\n",
    "print(unif_imdb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# IMDB binary bag of words\n",
    "\n",
    "binary_bag_train = binary_bag_of_words(wr_imdb_train_file)\n",
    "binary_bag_valid = binary_bag_of_words(wr_imdb_valid_file)\n",
    "binary_bag_test = binary_bag_of_words(wr_imdb_test_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get data ready for cross gridsearchCV with validation data\n",
    "\n",
    "imdb_train_valid_X = vstack([binary_bag_train, binary_bag_valid])\n",
    "train_test_fold = np.full((binary_bag_train.shape[0]), 0)\n",
    "valid_test_fold = np.full((binary_bag_valid.shape[0]), 1)\n",
    "test_fold = np.append(train_test_fold, valid_test_fold, axis=0)\n",
    "\n",
    "imdb_valid_Y = np.array(data_imdb_valid[:,-1], dtype=np.int32)\n",
    "\n",
    "imdb_train_valid_Y = np.array(np.append(imdb_train_Y, imdb_valid_Y, axis=0), dtype=np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BernoulliNB(alpha=0.96999999999999997, binarize=0.0, class_prior=None,\n",
      "      fit_prior=True)\n",
      "[ 0.83683619  0.8377214 ]\n"
     ]
    }
   ],
   "source": [
    "# Naive Bayes for IMDB Binary Bag of Words\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import PredefinedSplit\n",
    "\n",
    "clf = BernoulliNB()\n",
    "tuned_parameters = [{'alpha': np.arange(0.01, 1.01, 0.01)}]\n",
    "ps = PredefinedSplit(test_fold=test_fold)\n",
    "\n",
    "clf = GridSearchCV(clf, tuned_parameters, cv=ps, refit=True)\n",
    "clf.fit(imdb_train_valid_X, imdb_train_valid_Y)\n",
    "\n",
    "y_pred = clf.predict(binary_bag_test)\n",
    "\n",
    "print(clf.best_estimator_)\n",
    "print(metrics.f1_score(np.array(data_imdb_test[:,-1].flatten(), dtype=np.int32), y_pred, average=None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.87187953  0.87354952]\n",
      "LinearSVC(C=0.01, class_weight=None, dual=False, fit_intercept=True,\n",
      "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
      "     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
      "     verbose=0)\n"
     ]
    }
   ],
   "source": [
    "# Linear SVC/SVM\n",
    "from sklearn.svm import LinearSVC\n",
    "clf = LinearSVC()\n",
    "\n",
    "# We have 10000 features and 25000 in training + valid; so solve primal (which can't be used with hinge loss)\n",
    "tuned_parameters = [\n",
    "    {'penalty': ['l2'], \n",
    "     'loss':['squared_hinge'], \n",
    "     'dual':[False], \n",
    "    'C':np.arange(0.01, 1.01, 0.01),\n",
    "    }]\n",
    "ps = PredefinedSplit(test_fold=test_fold)\n",
    "\n",
    "clf = GridSearchCV(clf, tuned_parameters, cv=ps, refit=True)\n",
    "clf.fit(imdb_train_valid_X, imdb_train_valid_Y)\n",
    "\n",
    "y_pred = clf.predict(binary_bag_test)\n",
    "\n",
    "print(metrics.f1_score(np.array(data_imdb_test[:,-1].flatten(), dtype=np.int32), y_pred, average=None))\n",
    "print(clf.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.70212425  0.70331337]\n",
      "DecisionTreeClassifier(class_weight=None, criterion='entropy', max_depth=None,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='best')\n"
     ]
    }
   ],
   "source": [
    "# Decision tree for yelp binary bag of words\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "clf = DecisionTreeClassifier()\n",
    "\n",
    "tuned_parameters = [\n",
    "    {'criterion': ['gini','entropy'],\n",
    "#      'max_depth':[4,5,6,7,8,9,10,11,12,15,20,30,40,50,70,90,120,150,None]\n",
    "    }]\n",
    "ps = PredefinedSplit(test_fold=test_fold)\n",
    "\n",
    "clf = GridSearchCV(clf, tuned_parameters, cv=ps, refit=True)\n",
    "clf.fit(imdb_train_valid_X, imdb_train_valid_Y)\n",
    "\n",
    "y_pred = clf.predict(binary_bag_test)\n",
    "\n",
    "print(metrics.f1_score(np.array(data_imdb_test[:,-1].flatten(), dtype=np.int32), y_pred, average=None))\n",
    "print(clf.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Yelp frequency bag of words\n",
    "freq_bag_train = frequency_bag_of_words(wr_imdb_train_file)\n",
    "freq_bag_valid = frequency_bag_of_words(wr_imdb_valid_file)\n",
    "freq_bag_test = frequency_bag_of_words(wr_imdb_test_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get data ready for cross gridsearchCV with validation data\n",
    "\n",
    "imdb_train_valid_X = vstack([freq_bag_train, freq_bag_valid])\n",
    "train_test_fold = np.full((freq_bag_train.shape[0]), 0)\n",
    "valid_test_fold = np.full((freq_bag_valid.shape[0]), 1)\n",
    "test_fold = np.append(train_test_fold, valid_test_fold, axis=0)\n",
    "\n",
    "imdb_valid_Y = np.array(data_imdb_valid[:,-1], dtype=np.int32)\n",
    "\n",
    "imdb_train_valid_Y = np.array(np.append(imdb_train_Y, imdb_valid_Y, axis=0), dtype=np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GaussianNB(priors=None)\n",
      "[ 0.65187433  0.66867047]\n"
     ]
    }
   ],
   "source": [
    "# Naive Bayes for IMDB Binary Bag of Words\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "clf = GaussianNB()\n",
    "tuned_parameters = [{}]\n",
    "ps = PredefinedSplit(test_fold=test_fold)\n",
    "\n",
    "clf = GridSearchCV(clf, tuned_parameters, cv=ps, refit=True)\n",
    "clf.fit(imdb_train_valid_X.todense(), imdb_train_valid_Y)\n",
    "\n",
    "y_pred = clf.predict(freq_bag_test.todense())\n",
    "\n",
    "print(clf.best_estimator_)\n",
    "print(metrics.f1_score(np.array(data_imdb_test[:,-1].flatten(), dtype=np.int32), y_pred, average=None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearSVC(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
      "     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
      "     verbose=0)\n",
      "[ 0.81205246  0.81417548]\n"
     ]
    }
   ],
   "source": [
    "# Linear SVC/SVM\n",
    "from sklearn.svm import LinearSVC\n",
    "clf = LinearSVC()\n",
    "\n",
    "# We have 10000 features and 25000 in training + valid; so solve primal (can't use hinge loss with primal problem)\n",
    "tuned_parameters = [\n",
    "    {'penalty': ['l2'], \n",
    "     'loss':['squared_hinge'], \n",
    "     'dual':[False], \n",
    "    'C':np.arange(0.01, 1.01, 0.01),\n",
    "    }]\n",
    "ps = PredefinedSplit(test_fold=test_fold)\n",
    "\n",
    "clf = GridSearchCV(clf, tuned_parameters, cv=ps, refit=True)\n",
    "clf.fit(imdb_train_valid_X, imdb_train_valid_Y)\n",
    "\n",
    "y_pred = clf.predict(freq_bag_test)\n",
    "\n",
    "print(clf.best_estimator_)\n",
    "print(metrics.f1_score(np.array(data_imdb_test[:,-1].flatten(), dtype=np.int32), y_pred, average=None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.70260579  0.70026504]\n",
      "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='best')\n"
     ]
    }
   ],
   "source": [
    "# Decision tree for yelp binary bag of words\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "clf = DecisionTreeClassifier()\n",
    "\n",
    "tuned_parameters = [\n",
    "    {'criterion': ['gini','entropy'],\n",
    "#      'max_depth':[4,5,6,7,8,9,10,11,12,15,20,30,40,50,70,90,120,150,None]\n",
    "    }]\n",
    "ps = PredefinedSplit(test_fold=test_fold)\n",
    "\n",
    "clf = GridSearchCV(clf, tuned_parameters, cv=ps, refit=True)\n",
    "clf.fit(imdb_train_valid_X, imdb_train_valid_Y)\n",
    "\n",
    "y_pred = clf.predict(freq_bag_test)\n",
    "\n",
    "print(metrics.f1_score(np.array(data_imdb_test[:,-1].flatten(), dtype=np.int32), y_pred, average=None))\n",
    "print(clf.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
