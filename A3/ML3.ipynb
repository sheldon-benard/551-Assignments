{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A3 - NLP for Yelp and IMDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import os.path\n",
    "import re\n",
    "import numpy as np\n",
    "import string\n",
    "from sklearn import metrics\n",
    "from scipy.sparse import lil_matrix\n",
    "from scipy.sparse import vstack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Functions to open files\n",
    "def open_with_panda(filename):\n",
    "    return pd.read_csv(filename, sep='\\t', names=[\"Comment\", \"Evaluation\"]).as_matrix()\n",
    "\n",
    "\n",
    "def open_vocab_file(filename):\n",
    "    return pd.read_csv(yelp_vocab_file, sep='\\t', header=None, skip_blank_lines=True).as_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function to generate encodings\n",
    "def encode_reviews(file_to_create, X, Y, vocabFile, overwrite=False):\n",
    "    if not (os.path.isfile(file_to_create)) or overwrite:\n",
    "        # Grab vocab encodings\n",
    "        vocab = open_vocab_file(vocabFile)\n",
    "        vocab_dict = dict([(vocab[i][0], int(vocab[i][1])) for i in range(len(vocab))])\n",
    "        list_to_write = []\n",
    "\n",
    "        cleaned_reviews = [re.sub(r'[^\\w\\s\\d]|_','',X[i].lower()) for i in range(len(X))]\n",
    "        final_reviews = [re.sub(r'\\s+',' ',cleaned_reviews[i]).strip() for i in range(len(cleaned_reviews))]\n",
    "\n",
    "        for i in range(len(final_reviews)):\n",
    "            build_string = \"\"\n",
    "            for x in final_reviews[i].split(\" \"):\n",
    "                if x in vocab_dict.keys():\n",
    "                    build_string += str(vocab_dict[x]) + \" \"\n",
    "            list_to_write.append(build_string.strip() + \"\\t\" + str(Y[i]))\n",
    "\n",
    "        with open(file_to_create,'w') as f:\n",
    "            f.write(\"\\n\".join(list_to_write))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function to generate vocab\n",
    "def vocabulary(file, reviews, overwrite=False):\n",
    "    if not (os.path.isfile(file)) or overwrite:\n",
    "        # Let's create the vocab.txt file\n",
    "        cleaned_reviews = [re.sub(r'[^\\w\\s\\d]|_','',reviews[i].lower()) for i in range(len(reviews))]\n",
    "        final_reviews = [re.sub(r'\\s+',' ',cleaned_reviews[i]).strip() for i in range(len(cleaned_reviews))]\n",
    "\n",
    "        found_punct = False\n",
    "        for c in string.punctuation:\n",
    "            for rev in final_reviews:\n",
    "                if c in rev:\n",
    "                    # print(rev + \"\\n\")\n",
    "                    found_punct = True\n",
    "\n",
    "        if found_punct:\n",
    "            raise Exception('Found punctuation')\n",
    "\n",
    "        words = \" \".join(final_reviews).split(\" \")\n",
    "        unique_words, count = np.unique(words, return_counts=True)\n",
    "        unique_words_array = np.asarray((unique_words,count)).T\n",
    "        frequency_unique_words = np.flip(unique_words_array[unique_words_array[:,-1].astype(int).argsort()],0)\n",
    "\n",
    "        with open(file,'w') as f:\n",
    "            for i in range(10000):\n",
    "                f.write(\n",
    "                    frequency_unique_words[i][0] + \"\\t\" +\n",
    "                    str(i) + \"\\t\" +\n",
    "                    frequency_unique_words[i][-1] + \"\\n\"\n",
    "                )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# File paths\n",
    "\n",
    "yelp_train_file = \"hwk3_datasets/yelp-train.txt\"\n",
    "yelp_valid_file = \"hwk3_datasets/yelp-valid.txt\"\n",
    "yelp_test_file = \"hwk3_datasets/yelp-test.txt\"\n",
    "\n",
    "imdb_valid_file = \"hwk3_datasets/IMDB-valid.txt\"\n",
    "imdb_test_file = \"hwk3_datasets/IMDB-test.txt\"\n",
    "imdb_train_file = \"hwk3_datasets/IMDB-train.txt\"\n",
    "\n",
    "yelp_vocab_file = \"data/yelp-vocab.txt\"\n",
    "imdb_vocab_file = \"data/IMDB-vocab.txt\"\n",
    "\n",
    "wr_yelp_train_file = \"data/yelp-train.txt\"\n",
    "wr_yelp_valid_file = \"data/yelp-valid.txt\"\n",
    "wr_yelp_test_file = \"data/yelp-test.txt\"\n",
    "\n",
    "wr_imdb_valid_file = \"data/IMDB-valid.txt\"\n",
    "wr_imdb_test_file = \"data/IMDB-test.txt\"\n",
    "wr_imdb_train_file = \"data/IMDB-train.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Training data\n",
    "\n",
    "data_yelp_train = open_with_panda(yelp_train_file)\n",
    "yelp_train_X = data_yelp_train[:,0]\n",
    "yelp_train_Y = data_yelp_train[:,-1]\n",
    "\n",
    "data_imdb_train = open_with_panda(imdb_train_file)\n",
    "imdb_train_X = data_imdb_train[:,0]\n",
    "imdb_train_Y = data_imdb_train[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Generate vocab\n",
    "vocabulary(yelp_vocab_file, yelp_train_X.flatten())\n",
    "vocabulary(imdb_vocab_file, imdb_train_X.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Encode yelp reviews\n",
    "\n",
    "encode_reviews(wr_yelp_train_file, yelp_train_X, yelp_train_Y, yelp_vocab_file)\n",
    "\n",
    "data_yelp_valid = open_with_panda(yelp_valid_file)\n",
    "data_yelp_test = open_with_panda(yelp_test_file)\n",
    "\n",
    "encode_reviews(wr_yelp_valid_file, data_yelp_valid[:,0], data_yelp_valid[:,-1], yelp_vocab_file)\n",
    "encode_reviews(wr_yelp_test_file, data_yelp_test[:,0], data_yelp_test[:,-1], yelp_vocab_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Encode IMDB reviews\n",
    "encode_reviews(wr_imdb_train_file, imdb_train_X, imdb_train_Y, imdb_vocab_file)\n",
    "data_imdb_valid = open_with_panda(imdb_valid_file)\n",
    "data_imdb_test = open_with_panda(imdb_test_file)\n",
    "\n",
    "encode_reviews(wr_imdb_valid_file, data_imdb_valid[:,0], data_imdb_valid[:,-1], imdb_vocab_file)\n",
    "encode_reviews(wr_imdb_test_file, data_imdb_test[:,0], data_imdb_test[:,-1], imdb_vocab_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function - unif classifier\n",
    "def uniform_classifier(classes, y_test):\n",
    "    from random import randint\n",
    "    l = []\n",
    "    for i in range(len(y_test)):\n",
    "        rand = randint(0, len(classes)-1)\n",
    "        l.append(classes[rand])\n",
    "    y_pred = np.array(l, dtype=np.int16)\n",
    "    y_test = np.array(y_test, dtype=np.int16)\n",
    "    return metrics.f1_score(y_test, y_pred, average=None) # Get F1 score for each of the classes, thus Average=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function - majority classifier\n",
    "def majority_classifier(classes, y_train, y_test):\n",
    "    counter = dict([(classes[i], 0) for i in range(len(classes))])\n",
    "\n",
    "    for y in y_train:\n",
    "        counter[y]+=1\n",
    "\n",
    "    majority_class = max(counter, key=counter.get)\n",
    "    y_pred = np.full((len(y_test),1), majority_class, dtype=np.int16)\n",
    "    y_test = np.array(y_test, dtype=np.int16)\n",
    "    \n",
    "    return metrics.f1_score(y_test, y_pred, average=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Binary bag of words: read encoding file and generate\n",
    "def binary_bag_of_words(filename):\n",
    "    features = 10000\n",
    "    with open(filename, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        X = lil_matrix((len(lines), features), dtype=np.int32)\n",
    "        for i in range(len(lines)):\n",
    "            encodings = lines[i].split(\" \")\n",
    "            encodings[-1] = encodings[-1].split(\"\\t\")[0]\n",
    "\n",
    "            if len(encodings) == 1 and encodings[0] == '':\n",
    "                continue  # We have only 1 encoding, which means the review has no vocab words\n",
    "\n",
    "            for j in range(len(encodings)):\n",
    "                e = int(encodings[j])\n",
    "                X[i,e] = 1\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Frequency bag of words - with division\n",
    "def frequency_bag_of_words(filename):\n",
    "    features = 10000\n",
    "    with open(filename, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        X = lil_matrix((len(lines), features), dtype=np.int32)\n",
    "        for i in range(len(lines)):\n",
    "            encodings = lines[i].split(\" \")\n",
    "            encodings[-1] = encodings[-1].split(\"\\t\")[0]\n",
    "            \n",
    "            if len(encodings) == 1 and encodings[0] == '':\n",
    "                continue  # We have only 1 encoding, which means the review has no vocab words\n",
    "            \n",
    "            for j in range(len(encodings)):\n",
    "                e = int(encodings[j])\n",
    "                X[i,e] += 1\n",
    "        sum_vector = X.sum(axis=1)\n",
    "        for i in range(len(sum_vector)):\n",
    "            if sum_vector[i] == 0:\n",
    "                sum_vector[i] = 1  #just so we don't divide by 0\n",
    "\n",
    "        return lil_matrix(X / sum_vector, dtype=np.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "yelp_classes = [i for i in range(1,6)]\n",
    "imdb_classes = [0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test\n",
      "\n",
      "[ 0.11347518  0.12828947  0.16239316  0.26102941  0.22736031]\n",
      "0.178509506686\n",
      "[ 0.         0.         0.         0.5196151  0.       ]\n",
      "0.103923019985\n",
      "\n",
      "Training\n",
      "\n",
      "[ 0.12863706  0.12445309  0.16195698  0.26178279  0.2518199 ]\n",
      "0.185729962271\n",
      "[ 0.          0.          0.          0.52133502  0.        ]\n",
      "0.104267004647\n",
      "\n",
      "Validation\n",
      "\n",
      "[ 0.13986014  0.11636364  0.22047244  0.27797834  0.24206349]\n",
      "0.199347609716\n",
      "[ 0.          0.          0.          0.52507375  0.        ]\n",
      "0.105014749263\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nTest\\n\")\n",
    "unif = uniform_classifier(yelp_classes, data_yelp_test[:,-1].flatten())\n",
    "maj = majority_classifier(yelp_classes, yelp_train_Y, data_yelp_test[:,-1].flatten()) # Will get an F1 error, since we'll have 0/0 in some cases\n",
    "\n",
    "print(unif)\n",
    "print(np.mean(unif))\n",
    "\n",
    "print(maj)\n",
    "print(np.mean(maj))\n",
    "\n",
    "print(\"\\nTraining\\n\")\n",
    "unif = uniform_classifier(yelp_classes, yelp_train_Y)\n",
    "maj = majority_classifier(yelp_classes, yelp_train_Y, yelp_train_Y) # Will get an F1 error, since we'll have 0/0 in some cases\n",
    "\n",
    "print(unif)\n",
    "print(np.mean(unif))\n",
    "\n",
    "print(maj)\n",
    "print(np.mean(maj))\n",
    "\n",
    "print(\"\\nValidation\\n\")\n",
    "unif = uniform_classifier(yelp_classes, data_yelp_valid[:,-1].flatten())\n",
    "maj = majority_classifier(yelp_classes, yelp_train_Y, data_yelp_valid[:,-1].flatten()) # Will get an F1 error, since we'll have 0/0 in some cases\n",
    "\n",
    "print(unif)\n",
    "print(np.mean(unif))\n",
    "\n",
    "print(maj)\n",
    "print(np.mean(maj))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Yelp Binary Bag of Words\n",
    "\n",
    "binary_bag_train = binary_bag_of_words(wr_yelp_train_file)\n",
    "binary_bag_valid = binary_bag_of_words(wr_yelp_valid_file)\n",
    "binary_bag_test = binary_bag_of_words(wr_yelp_test_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get data ready for cross gridsearchCV with validation data\n",
    "\n",
    "yelp_train_valid_X = vstack([binary_bag_train, binary_bag_valid])\n",
    "train_test_fold = np.full((binary_bag_train.shape[0]), 0)\n",
    "valid_test_fold = np.full((binary_bag_valid.shape[0]), 1)\n",
    "test_fold = np.append(train_test_fold, valid_test_fold, axis=0)\n",
    "\n",
    "yelp_valid_Y = np.array(data_yelp_valid[:,-1], dtype=np.int32)\n",
    "\n",
    "yelp_train_valid_Y = np.array(np.append(yelp_train_Y, yelp_valid_Y, axis=0), dtype=np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BernoulliNB(alpha=0.029999999999999999, binarize=0.0, class_prior=None,\n",
      "      fit_prior=True)\n",
      "\n",
      "Test\n",
      "\n",
      "[ 0.416       0.26380368  0.24045802  0.4469526   0.52705283]\n",
      "0.378853424955\n",
      "\n",
      "Training\n",
      "\n",
      "[ 0.78423237  0.75395431  0.73790776  0.70096463  0.70933056]\n",
      "0.737277924229\n",
      "\n",
      "Validation\n",
      "\n",
      "[ 0.73939394  0.70857143  0.70138889  0.6539075   0.61744966]\n",
      "0.684142283459\n"
     ]
    }
   ],
   "source": [
    "# Naive Bayes for Yelp Binary Bag of Words\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import PredefinedSplit\n",
    "\n",
    "clf = BernoulliNB()\n",
    "tuned_parameters = [{'alpha': np.arange(0.01, 10.01, 0.01)}]\n",
    "ps = PredefinedSplit(test_fold=test_fold)\n",
    "\n",
    "clf = GridSearchCV(clf, tuned_parameters, cv=ps, refit=True, scoring='f1_micro')\n",
    "clf.fit(yelp_train_valid_X, yelp_train_valid_Y)\n",
    "\n",
    "print(clf.best_estimator_)\n",
    "\n",
    "print(\"\\nTest\\n\")\n",
    "\n",
    "y_pred = clf.predict(binary_bag_test)\n",
    "f1 = metrics.f1_score(np.array(data_yelp_test[:,-1].flatten(), dtype=np.int32), y_pred, average=None)\n",
    "print(f1)\n",
    "print(np.mean(f1))\n",
    "\n",
    "print(\"\\nTraining\\n\")\n",
    "\n",
    "y_pred = clf.predict(binary_bag_train)\n",
    "f1 = metrics.f1_score(np.array(yelp_train_Y, dtype=np.int32), y_pred, average=None)\n",
    "print(f1)\n",
    "print(np.mean(f1))\n",
    "\n",
    "print(\"\\nValidation\\n\")\n",
    "\n",
    "y_pred = clf.predict(binary_bag_valid)\n",
    "f1 = metrics.f1_score(np.array(yelp_valid_Y, dtype=np.int32), y_pred, average=None)\n",
    "print(f1)\n",
    "print(np.mean(f1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearSVC(C=0.10000000000000001, class_weight=None, dual=True,\n",
      "     fit_intercept=True, intercept_scaling=1, loss='hinge', max_iter=1000,\n",
      "     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
      "     verbose=0)\n",
      "\n",
      "Test\n",
      "\n",
      "[ 0.51803279  0.31454006  0.31751825  0.49215407  0.58948864]\n",
      "0.446346759278\n",
      "\n",
      "Training\n",
      "\n",
      "[ 0.94777563  0.93354684  0.89937107  0.8874308   0.89497161]\n",
      "0.91261918937\n",
      "\n",
      "Validation\n",
      "\n",
      "[ 0.97590361  0.96808511  0.91909385  0.89615932  0.89274448]\n",
      "0.930397273736\n"
     ]
    }
   ],
   "source": [
    "# Linear SVC/SVM\n",
    "from sklearn.svm import LinearSVC\n",
    "clf = LinearSVC()\n",
    "\n",
    "# We have 10000 features and 8000 in training + valid; so solve dual\n",
    "tuned_parameters = [\n",
    "    {'penalty': ['l2'], \n",
    "     'loss':['squared_hinge', 'hinge'], \n",
    "     'dual':[True], \n",
    "     'C': np.arange(0.1, 0.2, 0.1),\n",
    "#     'C':np.arange(0.1, 4.1, 0.1)\n",
    "    }]\n",
    "ps = PredefinedSplit(test_fold=test_fold)\n",
    "\n",
    "clf = GridSearchCV(clf, tuned_parameters, cv=ps, refit=True, scoring='f1_micro')\n",
    "clf.fit(yelp_train_valid_X, yelp_train_valid_Y)\n",
    "\n",
    "print(clf.best_estimator_)\n",
    "\n",
    "print(\"\\nTest\\n\")\n",
    "\n",
    "y_pred = clf.predict(binary_bag_test)\n",
    "f1 = metrics.f1_score(np.array(data_yelp_test[:,-1].flatten(), dtype=np.int32), y_pred, average=None)\n",
    "print(f1)\n",
    "print(np.mean(f1))\n",
    "\n",
    "print(\"\\nTraining\\n\")\n",
    "\n",
    "y_pred = clf.predict(binary_bag_train)\n",
    "f1 = metrics.f1_score(np.array(yelp_train_Y, dtype=np.int32), y_pred, average=None)\n",
    "print(f1)\n",
    "print(np.mean(f1))\n",
    "\n",
    "print(\"\\nValidation\\n\")\n",
    "\n",
    "y_pred = clf.predict(binary_bag_valid)\n",
    "f1 = metrics.f1_score(np.array(yelp_valid_Y, dtype=np.int32), y_pred, average=None)\n",
    "print(f1)\n",
    "print(np.mean(f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=9, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='random')\n",
      "\n",
      "Test\n",
      "\n",
      "[ 0.20080321  0.22155689  0.20332717  0.42153644  0.45232816]\n",
      "0.299910374372\n",
      "\n",
      "Training\n",
      "\n",
      "[ 0.51836306  0.45200698  0.49058756  0.69106938  0.71034483]\n",
      "0.572474362363\n",
      "\n",
      "Validation\n",
      "\n",
      "[ 0.55345912  0.40718563  0.52264808  0.69736842  0.69856459]\n",
      "0.575845169243\n"
     ]
    }
   ],
   "source": [
    "# Decision tree for yelp binary bag of words\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "clf = DecisionTreeClassifier()\n",
    "\n",
    "tuned_parameters = [\n",
    "    {'criterion': ['gini','entropy'],\n",
    "     'splitter':['best','random'],\n",
    "#      'max_depth':[None,10,20,30,40,50,60,70,80,90],\n",
    "     'max_depth':[None],\n",
    "#      'min_samples_leaf':[1,2,3,4,5,6,7,8,9,10,20,30,40,50],\n",
    "     'min_samples_leaf':[9],\n",
    "#      'max_features': [None,3,5,7,9,10,15,20],  \n",
    "    }]\n",
    "ps = PredefinedSplit(test_fold=test_fold)\n",
    "\n",
    "clf = GridSearchCV(clf, tuned_parameters, cv=ps, refit=True, scoring='f1_micro')\n",
    "clf.fit(yelp_train_valid_X, yelp_train_valid_Y)\n",
    "\n",
    "print(clf.best_estimator_)\n",
    "\n",
    "print(\"\\nTest\\n\")\n",
    "\n",
    "y_pred = clf.predict(binary_bag_test)\n",
    "f1 = metrics.f1_score(np.array(data_yelp_test[:,-1].flatten(), dtype=np.int32), y_pred, average=None)\n",
    "print(f1)\n",
    "print(np.mean(f1))\n",
    "\n",
    "print(\"\\nTraining\\n\")\n",
    "\n",
    "y_pred = clf.predict(binary_bag_train)\n",
    "f1 = metrics.f1_score(np.array(yelp_train_Y, dtype=np.int32), y_pred, average=None)\n",
    "print(f1)\n",
    "print(np.mean(f1))\n",
    "\n",
    "print(\"\\nValidation\\n\")\n",
    "\n",
    "y_pred = clf.predict(binary_bag_valid)\n",
    "f1 = metrics.f1_score(np.array(yelp_valid_Y, dtype=np.int32), y_pred, average=None)\n",
    "print(f1)\n",
    "print(np.mean(f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Yelp frequency bag of words\n",
    "freq_bag_train = frequency_bag_of_words(wr_yelp_train_file)\n",
    "freq_bag_valid = frequency_bag_of_words(wr_yelp_valid_file)\n",
    "freq_bag_test = frequency_bag_of_words(wr_yelp_test_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get data ready for cross gridsearchCV with validation data\n",
    "\n",
    "yelp_train_valid_X = vstack([freq_bag_train, freq_bag_valid])\n",
    "train_test_fold = np.full((freq_bag_train.shape[0]), 0)\n",
    "valid_test_fold = np.full((freq_bag_valid.shape[0]), 1)\n",
    "test_fold = np.append(train_test_fold, valid_test_fold, axis=0)\n",
    "\n",
    "yelp_valid_Y = np.array(data_yelp_valid[:,-1], dtype=np.int32)\n",
    "\n",
    "yelp_train_valid_Y = np.array(np.append(yelp_train_Y, yelp_valid_Y, axis=0), dtype=np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GaussianNB(priors=None)\n",
      "\n",
      "Test\n",
      "\n",
      "[ 0.17940199  0.140625    0.21021021  0.33888487  0.37697161]\n",
      "0.249218735688\n",
      "\n",
      "Training\n",
      "\n",
      "[ 0.74095103  0.72258838  0.73378265  0.7920077   0.83705052]\n",
      "0.765276055511\n",
      "\n",
      "Validation\n",
      "\n",
      "[ 0.75675676  0.734375    0.67980296  0.72759227  0.79341865]\n",
      "0.738389125345\n"
     ]
    }
   ],
   "source": [
    "# Naive Bayes for Yelp Binary Bag of Words\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "clf = GaussianNB()\n",
    "tuned_parameters = [{}]\n",
    "ps = PredefinedSplit(test_fold=test_fold)\n",
    "\n",
    "clf = GridSearchCV(clf, tuned_parameters, cv=ps, refit=True,scoring='f1_micro')\n",
    "clf.fit(yelp_train_valid_X.todense(), yelp_train_valid_Y)\n",
    "\n",
    "# y_pred = clf.predict(freq_bag_test.todense())\n",
    "\n",
    "# print(clf.best_estimator_)\n",
    "# f1 = metrics.f1_score(np.array(data_yelp_test[:,-1].flatten(), dtype=np.int32), y_pred, average=None)\n",
    "# print(f1)\n",
    "# print(np.mean(f1))\n",
    "\n",
    "print(clf.best_estimator_)\n",
    "\n",
    "print(\"\\nTest\\n\")\n",
    "\n",
    "y_pred = clf.predict(freq_bag_test.todense())\n",
    "f1 = metrics.f1_score(np.array(data_yelp_test[:,-1].flatten(), dtype=np.int32), y_pred, average=None)\n",
    "print(f1)\n",
    "print(np.mean(f1))\n",
    "\n",
    "print(\"\\nTraining\\n\")\n",
    "\n",
    "y_pred = clf.predict(freq_bag_train.todense())\n",
    "f1 = metrics.f1_score(np.array(yelp_train_Y, dtype=np.int32), y_pred, average=None)\n",
    "print(f1)\n",
    "print(np.mean(f1))\n",
    "\n",
    "print(\"\\nValidation\\n\")\n",
    "\n",
    "y_pred = clf.predict(freq_bag_valid.todense())\n",
    "f1 = metrics.f1_score(np.array(yelp_valid_Y, dtype=np.int32), y_pred, average=None)\n",
    "print(f1)\n",
    "print(np.mean(f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearSVC(C=0.10000000000000001, class_weight=None, dual=True,\n",
      "     fit_intercept=True, intercept_scaling=1, loss='hinge', max_iter=1000,\n",
      "     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
      "     verbose=0)\n",
      "\n",
      "Test\n",
      "\n",
      "[ 0.43137255  0.25095057  0.14795918  0.428125    0.62099448]\n",
      "0.375880355635\n",
      "\n",
      "Training\n",
      "\n",
      "[ 0.63716814  0.52057613  0.40836941  0.50374404  0.65550466]\n",
      "0.545072476966\n",
      "\n",
      "Validation\n",
      "\n",
      "[ 0.62318841  0.60130719  0.43049327  0.50386399  0.62693683]\n",
      "0.557157937215\n"
     ]
    }
   ],
   "source": [
    "# Linear SVC/SVM\n",
    "from sklearn.svm import LinearSVC\n",
    "clf = LinearSVC()\n",
    "\n",
    "# We have 10000 features and 8000 in training + valid; so solve dual\n",
    "tuned_parameters = [\n",
    "    {'penalty': ['l2'], \n",
    "     'loss':['squared_hinge', 'hinge'], \n",
    "     'dual':[True], \n",
    "#      'C': np.arange(0.1, 0.2, 0.1),\n",
    "    'C':np.arange(0.1, 4.1, 0.1)\n",
    "    }]\n",
    "ps = PredefinedSplit(test_fold=test_fold)\n",
    "\n",
    "clf = GridSearchCV(clf, tuned_parameters, cv=ps, refit=True, scoring='f1_micro')\n",
    "clf.fit(yelp_train_valid_X, yelp_train_valid_Y)\n",
    "\n",
    "print(clf.best_estimator_)\n",
    "\n",
    "print(\"\\nTest\\n\")\n",
    "\n",
    "y_pred = clf.predict(freq_bag_test)\n",
    "f1 = metrics.f1_score(np.array(data_yelp_test[:,-1].flatten(), dtype=np.int32), y_pred, average=None)\n",
    "print(f1)\n",
    "print(np.mean(f1))\n",
    "\n",
    "print(\"\\nTraining\\n\")\n",
    "\n",
    "y_pred = clf.predict(freq_bag_train)\n",
    "f1 = metrics.f1_score(np.array(yelp_train_Y, dtype=np.int32), y_pred, average=None)\n",
    "print(f1)\n",
    "print(np.mean(f1))\n",
    "\n",
    "print(\"\\nValidation\\n\")\n",
    "\n",
    "y_pred = clf.predict(freq_bag_valid)\n",
    "f1 = metrics.f1_score(np.array(yelp_valid_Y, dtype=np.int32), y_pred, average=None)\n",
    "print(f1)\n",
    "print(np.mean(f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeClassifier(class_weight=None, criterion='entropy', max_depth=50,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=40, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='best')\n",
      "\n",
      "Test\n",
      "\n",
      "[ 0.192       0.15974441  0.18947368  0.43612903  0.45750708]\n",
      "0.286970841513\n",
      "\n",
      "Training\n",
      "\n",
      "[ 0.36926361  0.25855513  0.27848101  0.53763441  0.58487195]\n",
      "0.405761221837\n",
      "\n",
      "Validation\n",
      "\n",
      "[ 0.44444444  0.18055556  0.3         0.53506494  0.60843373]\n",
      "0.413699734001\n"
     ]
    }
   ],
   "source": [
    "# Decision tree for yelp binary bag of words\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "clf = DecisionTreeClassifier()\n",
    "\n",
    "tuned_parameters = [\n",
    "    {\n",
    "#   'criterion': ['gini','entropy'],\n",
    "    'criterion': ['entropy'],\n",
    "#      'splitter':['best','random'],\n",
    "        'splitter':['best'],\n",
    "     'max_depth':[50],\n",
    "#      'max_depth':[None,10,20,30,40,50,60,70,80,90],\n",
    "     'min_samples_leaf':[40],\n",
    "#      'min_samples_leaf':[1,2,3,4,5,6,7,8,9,10,20,30,40,50],\n",
    "     'max_features': [None],  \n",
    "    }]\n",
    "ps = PredefinedSplit(test_fold=test_fold)\n",
    "\n",
    "clf = GridSearchCV(clf, tuned_parameters, cv=ps, refit=True, scoring='f1_micro')\n",
    "clf.fit(yelp_train_valid_X, yelp_train_valid_Y)\n",
    "\n",
    "print(clf.best_estimator_)\n",
    "\n",
    "print(\"\\nTest\\n\")\n",
    "\n",
    "y_pred = clf.predict(freq_bag_test)\n",
    "f1 = metrics.f1_score(np.array(data_yelp_test[:,-1].flatten(), dtype=np.int32), y_pred, average=None)\n",
    "print(f1)\n",
    "print(np.mean(f1))\n",
    "\n",
    "print(\"\\nTraining\\n\")\n",
    "\n",
    "y_pred = clf.predict(freq_bag_train)\n",
    "f1 = metrics.f1_score(np.array(yelp_train_Y, dtype=np.int32), y_pred, average=None)\n",
    "print(f1)\n",
    "print(np.mean(f1))\n",
    "\n",
    "print(\"\\nValidation\\n\")\n",
    "\n",
    "y_pred = clf.predict(freq_bag_valid)\n",
    "f1 = metrics.f1_score(np.array(yelp_valid_Y, dtype=np.int32), y_pred, average=None)\n",
    "print(f1)\n",
    "print(np.mean(f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test\n",
      "\n",
      "[ 0.500618    0.49737148]\n",
      "0.498994740647\n",
      "\n",
      "Train\n",
      "\n",
      "[ 0.50490716  0.49959786]\n",
      "0.502252508516\n",
      "\n",
      "Validation\n",
      "\n",
      "[ 0.50019873  0.49376006]\n",
      "0.496979396276\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nTest\\n\")\n",
    "unif_imdb = uniform_classifier(imdb_classes, data_imdb_test[:,-1].flatten())\n",
    "\n",
    "print(unif_imdb)\n",
    "print(np.mean(unif_imdb))\n",
    "\n",
    "print(\"\\nTrain\\n\")\n",
    "unif_imdb = uniform_classifier(imdb_classes, imdb_train_Y)\n",
    "\n",
    "print(unif_imdb)\n",
    "print(np.mean(unif_imdb))\n",
    "\n",
    "print(\"\\nValidation\\n\")\n",
    "unif_imdb = uniform_classifier(imdb_classes, data_imdb_valid[:,-1].flatten())\n",
    "\n",
    "print(unif_imdb)\n",
    "print(np.mean(unif_imdb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# IMDB binary bag of words\n",
    "\n",
    "binary_bag_train = binary_bag_of_words(wr_imdb_train_file)\n",
    "binary_bag_valid = binary_bag_of_words(wr_imdb_valid_file)\n",
    "binary_bag_test = binary_bag_of_words(wr_imdb_test_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get data ready for cross gridsearchCV with validation data\n",
    "\n",
    "imdb_train_valid_X = vstack([binary_bag_train, binary_bag_valid])\n",
    "train_test_fold = np.full((binary_bag_train.shape[0]), 0)\n",
    "valid_test_fold = np.full((binary_bag_valid.shape[0]), 1)\n",
    "test_fold = np.append(train_test_fold, valid_test_fold, axis=0)\n",
    "\n",
    "imdb_valid_Y = np.array(data_imdb_valid[:,-1], dtype=np.int32)\n",
    "\n",
    "imdb_train_valid_Y = np.array(np.append(imdb_train_Y, imdb_valid_Y, axis=0), dtype=np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BernoulliNB(alpha=0.71000000000000008, binarize=0.0, class_prior=None,\n",
      "      fit_prior=True)\n",
      "\n",
      "Test\n",
      "\n",
      "[ 0.83681572  0.83766208]\n",
      "0.837238899735\n",
      "\n",
      "Training\n",
      "\n",
      "[ 0.85830014  0.86178969]\n",
      "0.860044915158\n",
      "\n",
      "Validation\n",
      "\n",
      "[ 0.85910792  0.8622565 ]\n",
      "0.860682210511\n"
     ]
    }
   ],
   "source": [
    "# Naive Bayes for IMDB Binary Bag of Words\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import PredefinedSplit\n",
    "\n",
    "clf = BernoulliNB()\n",
    "tuned_parameters = [{'alpha': np.arange(0.01, 10.01, 0.01)}]\n",
    "ps = PredefinedSplit(test_fold=test_fold)\n",
    "\n",
    "clf = GridSearchCV(clf, tuned_parameters, cv=ps, refit=True, scoring='f1')\n",
    "clf.fit(imdb_train_valid_X, imdb_train_valid_Y)\n",
    "\n",
    "print(clf.best_estimator_)\n",
    "\n",
    "print(\"\\nTest\\n\")\n",
    "\n",
    "y_pred = clf.predict(binary_bag_test)\n",
    "f1 = metrics.f1_score(np.array(data_imdb_test[:,-1].flatten(), dtype=np.int32), y_pred, average=None)\n",
    "print(f1)\n",
    "print(np.mean(f1))\n",
    "\n",
    "print(\"\\nTraining\\n\")\n",
    "\n",
    "y_pred = clf.predict(binary_bag_train)\n",
    "f1 = metrics.f1_score(np.array(imdb_train_Y, dtype=np.int32), y_pred, average=None)\n",
    "print(f1)\n",
    "print(np.mean(f1))\n",
    "\n",
    "print(\"\\nValidation\\n\")\n",
    "\n",
    "y_pred = clf.predict(binary_bag_valid)\n",
    "f1 = metrics.f1_score(np.array(imdb_valid_Y, dtype=np.int32), y_pred, average=None)\n",
    "print(f1)\n",
    "print(np.mean(f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearSVC(C=0.01, class_weight=None, dual=False, fit_intercept=True,\n",
      "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
      "     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
      "     verbose=0)\n",
      "\n",
      "Test\n",
      "\n",
      "[ 0.87187953  0.87354952]\n",
      "0.872714522448\n",
      "\n",
      "Training\n",
      "\n",
      "[ 0.93123825  0.93222134]\n",
      "0.931729794206\n",
      "\n",
      "Validation\n",
      "\n",
      "[ 0.93252891  0.93326703]\n",
      "0.932897970164\n"
     ]
    }
   ],
   "source": [
    "# Linear SVC/SVM\n",
    "from sklearn.svm import LinearSVC\n",
    "clf = LinearSVC()\n",
    "\n",
    "# We have 10000 features and 25000 in training + valid; so solve primal (which can't be used with hinge loss)\n",
    "tuned_parameters = [\n",
    "    {'penalty': ['l2'], \n",
    "     'loss':['squared_hinge'], \n",
    "     'dual':[False], \n",
    "#     'C':np.arange(0.01, 4.01, 0.01),\n",
    "     'C':np.arange(0.01, 0.21, 0.01),\n",
    "    }]\n",
    "ps = PredefinedSplit(test_fold=test_fold)\n",
    "\n",
    "clf = GridSearchCV(clf, tuned_parameters, cv=ps, refit=True, scoring='f1')\n",
    "clf.fit(imdb_train_valid_X, imdb_train_valid_Y)\n",
    "\n",
    "print(clf.best_estimator_)\n",
    "\n",
    "print(\"\\nTest\\n\")\n",
    "\n",
    "y_pred = clf.predict(binary_bag_test)\n",
    "f1 = metrics.f1_score(np.array(data_imdb_test[:,-1].flatten(), dtype=np.int32), y_pred, average=None)\n",
    "print(f1)\n",
    "print(np.mean(f1))\n",
    "\n",
    "print(\"\\nTraining\\n\")\n",
    "\n",
    "y_pred = clf.predict(binary_bag_train)\n",
    "f1 = metrics.f1_score(np.array(imdb_train_Y, dtype=np.int32), y_pred, average=None)\n",
    "print(f1)\n",
    "print(np.mean(f1))\n",
    "\n",
    "print(\"\\nValidation\\n\")\n",
    "\n",
    "y_pred = clf.predict(binary_bag_valid)\n",
    "f1 = metrics.f1_score(np.array(imdb_valid_Y, dtype=np.int32), y_pred, average=None)\n",
    "print(f1)\n",
    "print(np.mean(f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeClassifier(class_weight=None, criterion='entropy', max_depth=20,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=40, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='random')\n",
      "\n",
      "Test\n",
      "\n",
      "[ 0.72195083  0.71794045]\n",
      "0.719945642845\n",
      "\n",
      "Training\n",
      "\n",
      "[ 0.73056485  0.7216914 ]\n",
      "0.726128125065\n",
      "\n",
      "Validation\n",
      "\n",
      "[ 0.72851081  0.72119185]\n",
      "0.724851328952\n"
     ]
    }
   ],
   "source": [
    "# Decision tree for imdb binary bag of words\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "clf = DecisionTreeClassifier()\n",
    "\n",
    "tuned_parameters = [\n",
    "    {\n",
    "  'criterion': ['gini','entropy'],\n",
    "     'splitter':['best','random'],\n",
    "     'max_depth':[None,10,20,30,40,50,60,70,80,90],\n",
    "     'min_samples_leaf':[1,2,3,4,5,6,7,8,9,10,20,30,40,50],\n",
    "     'max_features': [None],  \n",
    "    }]\n",
    "ps = PredefinedSplit(test_fold=test_fold)\n",
    "\n",
    "clf = GridSearchCV(clf, tuned_parameters, cv=ps, refit=True, scoring='f1')\n",
    "clf.fit(imdb_train_valid_X, imdb_train_valid_Y)\n",
    "\n",
    "print(clf.best_estimator_)\n",
    "\n",
    "print(\"\\nTest\\n\")\n",
    "\n",
    "y_pred = clf.predict(binary_bag_test)\n",
    "f1 = metrics.f1_score(np.array(data_imdb_test[:,-1].flatten(), dtype=np.int32), y_pred, average=None)\n",
    "print(f1)\n",
    "print(np.mean(f1))\n",
    "\n",
    "print(\"\\nTraining\\n\")\n",
    "\n",
    "y_pred = clf.predict(binary_bag_train)\n",
    "f1 = metrics.f1_score(np.array(imdb_train_Y, dtype=np.int32), y_pred, average=None)\n",
    "print(f1)\n",
    "print(np.mean(f1))\n",
    "\n",
    "print(\"\\nValidation\\n\")\n",
    "\n",
    "y_pred = clf.predict(binary_bag_valid)\n",
    "f1 = metrics.f1_score(np.array(imdb_valid_Y, dtype=np.int32), y_pred, average=None)\n",
    "print(f1)\n",
    "print(np.mean(f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Yelp frequency bag of words\n",
    "freq_bag_train = frequency_bag_of_words(wr_imdb_train_file)\n",
    "freq_bag_valid = frequency_bag_of_words(wr_imdb_valid_file)\n",
    "freq_bag_test = frequency_bag_of_words(wr_imdb_test_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get data ready for cross gridsearchCV with validation data\n",
    "\n",
    "imdb_train_valid_X = vstack([freq_bag_train, freq_bag_valid])\n",
    "train_test_fold = np.full((freq_bag_train.shape[0]), 0)\n",
    "valid_test_fold = np.full((freq_bag_valid.shape[0]), 1)\n",
    "test_fold = np.append(train_test_fold, valid_test_fold, axis=0)\n",
    "\n",
    "imdb_valid_Y = np.array(data_imdb_valid[:,-1], dtype=np.int32)\n",
    "\n",
    "imdb_train_valid_Y = np.array(np.append(imdb_train_Y, imdb_valid_Y, axis=0), dtype=np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GaussianNB(priors=None)\n",
      "\n",
      "Test\n",
      "\n",
      "[ 0.65187433  0.66867047]\n",
      "0.660272399802\n",
      "\n",
      "Training\n",
      "\n",
      "[ 0.7930265   0.81047254]\n",
      "0.801749520405\n",
      "\n",
      "Validation\n",
      "\n",
      "[ 0.79233361  0.812262  ]\n",
      "0.802297804792\n"
     ]
    }
   ],
   "source": [
    "# Naive Bayes for IMDB Binary Bag of Words\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "clf = GaussianNB()\n",
    "tuned_parameters = [{}]\n",
    "ps = PredefinedSplit(test_fold=test_fold)\n",
    "\n",
    "clf = GridSearchCV(clf, tuned_parameters, cv=ps, refit=True, scoring='f1')\n",
    "clf.fit(imdb_train_valid_X.todense(), imdb_train_valid_Y)\n",
    "\n",
    "# y_pred = clf.predict(freq_bag_test.todense())\n",
    "\n",
    "print(clf.best_estimator_)\n",
    "\n",
    "print(\"\\nTest\\n\")\n",
    "\n",
    "y_pred = clf.predict(freq_bag_test.todense())\n",
    "f1 = metrics.f1_score(np.array(data_imdb_test[:,-1].flatten(), dtype=np.int32), y_pred, average=None)\n",
    "print(f1)\n",
    "print(np.mean(f1))\n",
    "\n",
    "print(\"\\nTraining\\n\")\n",
    "\n",
    "y_pred = clf.predict(freq_bag_train.todense())\n",
    "f1 = metrics.f1_score(np.array(imdb_train_Y, dtype=np.int32), y_pred, average=None)\n",
    "print(f1)\n",
    "print(np.mean(f1))\n",
    "\n",
    "print(\"\\nValidation\\n\")\n",
    "\n",
    "y_pred = clf.predict(freq_bag_valid.todense())\n",
    "f1 = metrics.f1_score(np.array(imdb_valid_Y, dtype=np.int32), y_pred, average=None)\n",
    "print(f1)\n",
    "print(np.mean(f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearSVC(C=10.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
      "     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
      "     verbose=0)\n",
      "\n",
      "Test\n",
      "\n",
      "[ 0.86714636  0.86844724]\n",
      "0.867796799836\n",
      "\n",
      "Training\n",
      "\n",
      "[ 0.88369908  0.88522419]\n",
      "0.884461633815\n",
      "\n",
      "Validation\n",
      "\n",
      "[ 0.88048411  0.88249876]\n",
      "0.881491437756\n"
     ]
    }
   ],
   "source": [
    "# Linear SVC/SVM\n",
    "from sklearn.svm import LinearSVC\n",
    "clf = LinearSVC()\n",
    "\n",
    "# We have 10000 features and 25000 in training + valid; so solve primal (can't use hinge loss with primal problem)\n",
    "tuned_parameters = [\n",
    "    {'penalty': ['l2'], \n",
    "     'loss':['squared_hinge'], \n",
    "     'dual':[False], \n",
    "    'C':np.arange(0.5, 10.5, 0.5),\n",
    "    }]\n",
    "ps = PredefinedSplit(test_fold=test_fold)\n",
    "\n",
    "clf = GridSearchCV(clf, tuned_parameters, cv=ps, refit=True, scoring='f1')\n",
    "clf.fit(imdb_train_valid_X, imdb_train_valid_Y)\n",
    "\n",
    "print(clf.best_estimator_)\n",
    "\n",
    "print(\"\\nTest\\n\")\n",
    "\n",
    "y_pred = clf.predict(freq_bag_test)\n",
    "f1 = metrics.f1_score(np.array(data_imdb_test[:,-1].flatten(), dtype=np.int32), y_pred, average=None)\n",
    "print(f1)\n",
    "print(np.mean(f1))\n",
    "\n",
    "print(\"\\nTraining\\n\")\n",
    "\n",
    "y_pred = clf.predict(freq_bag_train)\n",
    "f1 = metrics.f1_score(np.array(imdb_train_Y, dtype=np.int32), y_pred, average=None)\n",
    "print(f1)\n",
    "print(np.mean(f1))\n",
    "\n",
    "print(\"\\nValidation\\n\")\n",
    "\n",
    "y_pred = clf.predict(freq_bag_valid)\n",
    "f1 = metrics.f1_score(np.array(imdb_valid_Y, dtype=np.int32), y_pred, average=None)\n",
    "print(f1)\n",
    "print(np.mean(f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=20,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='random')\n",
      "\n",
      "Test\n",
      "\n",
      "[ 0.6904073   0.74741238]\n",
      "0.718909842242\n",
      "\n",
      "Training\n",
      "\n",
      "[ 0.78822469  0.83097579]\n",
      "0.809600238642\n",
      "\n",
      "Validation\n",
      "\n",
      "[ 0.78618679  0.82790362]\n",
      "0.807045205524\n"
     ]
    }
   ],
   "source": [
    "# Decision tree for yelp binary bag of words\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "clf = DecisionTreeClassifier()\n",
    "\n",
    "tuned_parameters = [\n",
    "    {\n",
    "  'criterion': ['gini','entropy'],\n",
    "     'splitter':['best','random'],\n",
    "     'max_depth':[None,10,20,30,40,50,60,70,80,90],\n",
    "     'min_samples_leaf':[1,2,3,4,5,6,7,8,9,10,20,30,40,50],\n",
    "     'max_features': [None],  \n",
    "    }]\n",
    "ps = PredefinedSplit(test_fold=test_fold)\n",
    "\n",
    "clf = GridSearchCV(clf, tuned_parameters, cv=ps, refit=True, scoring='f1')\n",
    "clf.fit(imdb_train_valid_X, imdb_train_valid_Y)\n",
    "\n",
    "print(clf.best_estimator_)\n",
    "\n",
    "print(\"\\nTest\\n\")\n",
    "\n",
    "y_pred = clf.predict(freq_bag_test)\n",
    "f1 = metrics.f1_score(np.array(data_imdb_test[:,-1].flatten(), dtype=np.int32), y_pred, average=None)\n",
    "print(f1)\n",
    "print(np.mean(f1))\n",
    "\n",
    "print(\"\\nTraining\\n\")\n",
    "\n",
    "y_pred = clf.predict(freq_bag_train)\n",
    "f1 = metrics.f1_score(np.array(imdb_train_Y, dtype=np.int32), y_pred, average=None)\n",
    "print(f1)\n",
    "print(np.mean(f1))\n",
    "\n",
    "print(\"\\nValidation\\n\")\n",
    "\n",
    "y_pred = clf.predict(freq_bag_valid)\n",
    "f1 = metrics.f1_score(np.array(imdb_valid_Y, dtype=np.int32), y_pred, average=None)\n",
    "print(f1)\n",
    "print(np.mean(f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
